{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"kagglehub[pandas-datasets]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe199581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/gk1mgywn6vdbyd8bt3p57yrc0000gn/T/ipykernel_8238/3995960101.py:9: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/rohitsahoo/sales-forecasting?dataset_version_number=2&file_name=train.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 480k/480k [00:00<00:00, 605kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of train.csv...\n",
      "First 5 records:    Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
      "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City       State  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "\n",
      "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
      "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
      "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  \n",
      "0                  Bush Somerset Collection Bookcase  261.9600  \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"train.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"rohitsahoo/sales-forecasting\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86711a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# prep data for Apriori algorithm\n",
    "apriori_df = df[['Order ID', 'Sub-Category']].copy()\n",
    "apriori_df.dropna(subset=['Order ID', 'Sub-Category'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb269343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data to get counts of each Sub-Category per Order ID\n",
    "transaction_counts = apriori_df.groupby(['Order ID', 'Sub-Category'])['Sub-Category'].count().reset_index(name='Count')\n",
    "\n",
    "# pivot table to create the basket format\n",
    "basket = transaction_counts.pivot_table(\n",
    "    index='Order ID', \n",
    "    columns='Sub-Category', \n",
    "    values='Count', \n",
    "    aggfunc='sum'\n",
    ").fillna(0)\n",
    "\n",
    "# convert any value > 1 to 1 (to indicate presence/absence)\n",
    "def encode_units(x):\n",
    "    return 1 if x >= 1 else 0\n",
    "\n",
    "basket_sets = basket.applymap(encode_units)\n",
    "\n",
    "st.subheader(\"Transaction Basket (One-Hot Encoded)\")\n",
    "st.caption(\"Rows are Orders, Columns are Sub-Categories (1 = Present, 0 = Absent)\")\n",
    "st.dataframe(basket_sets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.header(\"Apriori Algorithm: Interactive Rules\")\n",
    "\n",
    "# sliders for parameters\n",
    "min_support = st.slider('Min Support (Frequency of itemset)', 0.001, 0.1, 0.005, 0.001)\n",
    "min_confidence = st.slider('Min Confidence (Likelihood of Consequent)', 0.1, 1.0, 0.7, 0.05)\n",
    "min_lift = st.slider('Min Lift (Strength of Rule)', 0.5, 5.0, 1.2, 0.1)\n",
    "\n",
    "# generate Frequent Itemsets ---\n",
    "frequent_itemsets = apriori(basket_sets, min_support=min_support, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "# generate Association Rules \n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_lift)\n",
    "\n",
    "# filter by confidence\n",
    "rules = rules[rules['confidence'] >= min_confidence]\n",
    "\n",
    "st.subheader(\"Discovered Association Rules\")\n",
    "st.write(f\"Found **{len(rules)}** rules with the current parameters.\")\n",
    "\n",
    "# Display rules with key metrics\n",
    "display_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('lift', ascending=False)\n",
    "\n",
    "# Convert frozensets to strings for better display\n",
    "display_rules['antecedents'] = display_rules['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "display_rules['consequents'] = display_rules['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "\n",
    "# Interactive filtering by item\n",
    "item_filter = st.text_input('Filter Rules by Consequent (e.g., \"Chairs\"):')\n",
    "if item_filter:\n",
    "    display_rules = display_rules[display_rules['consequents'].str.contains(item_filter, case=False, na=False)]\n",
    "\n",
    "st.dataframe(display_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
